{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A [simple] CNN for Image Classification\n",
    "\n",
    "As described in the `README`, we will begin with a simple CNN that classifies a single image as belong to a cheap, average, or expensive house. The model architecture is reproduced below:\n",
    "\n",
    "| Layer | Input Dimensions | Output Dimensions | Objective | Activation Function | In Channels | Out Channels | Kernel | Stride | Padding |\n",
    "|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------| \n",
    "| Convolutional Layer 1 | 224, 224, 3 | 224, 224, 32 | Learn spatial features | ReLU | 3 | 32 | 3 | 1 | 1\n",
    "| Pooling Layer 1 | 224, 224, 32 | 112, 112, 32 | Down-sampling | | 32 | 32 | 2 | 2 | 0\n",
    "| Convolutional Layer 2 | 112, 112, 32 | 112, 112, 64 | Learn spatial features | ReLU | 32 | 64 | 3 | 1 | 1\n",
    "| Pooling Layer 2 | 112, 112, 64 | 56, 56, 64 | Down-sampling | | 64 | 64 | 2 | 2 | 0\n",
    "| Dense Layer 1 | 200704 | 512 | Learn spatial features | ReLU | 200704 | 512 | | | |\n",
    "| Dense Layer 2 | 512 | 3 | Classification | Softmax | 512 | 3 | | | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from src.preprocessing import get_housing_dataset\n",
    "from src.logging import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN Model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            3, 32, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            32, 64, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.fc1 = nn.Linear(64 * 56 * 56, 512)\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(512, 3)  # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 56 * 56)  # Flatten the tensor for the dense layer\n",
    "        x = self.dropout1(x)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)  # No activation function here as CrossEntropyLoss will be used\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_housing_dataset()\n",
    "\n",
    "# Create indices for the full dataset and split them\n",
    "indices = list(range(len(dataset)))\n",
    "train_indices, val_indices, _, _ = train_test_split(\n",
    "    indices, indices, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_subset = Subset(dataset, train_indices)\n",
    "val_subset = Subset(dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=4, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_subset, batch_size=4, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 22:32:56,026 [INFO]: Epoch 1, Loss: 1.29, Training Accuracy: 0.33, Validation Accuracy: 0.32\n",
      "2024-02-19 22:36:37,639 [INFO]: Epoch 2, Loss: 1.10, Training Accuracy: 0.34, Validation Accuracy: 0.32\n",
      "2024-02-19 22:40:36,196 [INFO]: Epoch 3, Loss: 1.10, Training Accuracy: 0.34, Validation Accuracy: 0.32\n",
      "2024-02-19 22:43:51,318 [INFO]: Epoch 4, Loss: 1.10, Training Accuracy: 0.34, Validation Accuracy: 0.34\n",
      "2024-02-19 22:47:09,898 [INFO]: Epoch 5, Loss: 1.08, Training Accuracy: 0.39, Validation Accuracy: 0.36\n",
      "2024-02-19 22:50:35,477 [INFO]: Epoch 6, Loss: 1.07, Training Accuracy: 0.43, Validation Accuracy: 0.38\n",
      "2024-02-19 22:54:07,066 [INFO]: Epoch 7, Loss: 0.96, Training Accuracy: 0.53, Validation Accuracy: 0.38\n",
      "2024-02-19 22:57:25,070 [INFO]: Epoch 8, Loss: 0.73, Training Accuracy: 0.69, Validation Accuracy: 0.41\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)  # TODO try a higher weight decay\n",
    "\n",
    "# TODO apply learning rate decay\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Run a training epoch\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_training_accuracy = 0.0\n",
    "        running_validation_accuracy = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            outputs = model(images)  # Make prediction\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Compute gradients\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            # Compute loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            running_training_accuracy += accuracy_score(\n",
    "                labels.numpy(), predicted.numpy()\n",
    "            )\n",
    "\n",
    "        # Compare to validation accuracy,\n",
    "        # this slows things down but is useful for understanding\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                running_validation_accuracy += accuracy_score(\n",
    "                    labels.numpy(), predicted.numpy()\n",
    "                )\n",
    "\n",
    "        logger.info(\n",
    "            f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.2f}, \"\n",
    "            f\"Training Accuracy: {running_training_accuracy/len(train_loader):.2f}, \"\n",
    "            f\"Validation Accuracy: {running_validation_accuracy/len(val_loader):.2f}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# TODO make this work on a GPU and use Google colab?\n",
    "\n",
    "# Example training call (assuming train_loader is defined)\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "We interrupted the previous training block because the model has clearly just started to massively overfit to the training data, with marginal (or even negative) returns on the validation accuracy. So what can we do to reduce overfitting and ideally attain better validation accuracies?\n",
    "\n",
    "- Dropout regularization: we will add dropout layers to \n",
    "- \n",
    "\n",
    "*Notice that we are using accuracy as our score because our labels are perfectly balanced, so we don't need to worry about precision, recall, f1 scores, and all that jazz.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO plotting function to visualize predictions\n",
    "# TODO check the shape of the NN's outputs. Are they converging to [0.33, 0.33, 0.33] for most predictions?\n",
    "# If so then the model is not learning anything meaningful, and we need a different architecture on the output\n",
    "# layer or a different loss function.\n",
    "# TODO data augmentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, val_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():  # Don't waste resources on gradients\n",
    "        for images, _ in val_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "    return predictions\n",
    "\n",
    "\n",
    "predictions = predict(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: modify the dataset to treat each image as its own sample\n",
    "# thereby increasing our dataset size by 33x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overfitting, apply learning rate decay, and use early stopping\n",
    "# Try doing some hyperparameter tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract labels from test_loader\n",
    "labels = []\n",
    "for _, label in val_loader:\n",
    "    labels.extend(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31900826446280994"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fscore = f1_score(labels, predictions, average=None)\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "logger.info(f\"Accuracy: {accuracy}\")\n",
    "logger.info(f\"F1 Scores by class: {fscore}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract labels from test_loader\n",
    "labels = []\n",
    "for _, label in train_loader:\n",
    "    labels.extend(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 15:31:30,228 [INFO]: Accuracy: 0.3406456953642384\n",
      "2024-02-19 15:31:30,228 [INFO]: Accuracy: 0.3406456953642384\n",
      "2024-02-19 15:31:30,229 [INFO]: F1 Scores by class: [0.33925291 0.34868017 0.33375796]\n",
      "2024-02-19 15:31:30,229 [INFO]: F1 Scores by class: [0.33925291 0.34868017 0.33375796]\n"
     ]
    }
   ],
   "source": [
    "fscore = f1_score(labels, predictions, average=None)\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "logger.info(f\"Accuracy: {accuracy}\")\n",
    "logger.info(f\"F1 Scores by class: {fscore}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.0992163652615832\n"
     ]
    }
   ],
   "source": [
    "model_small = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# TODO make this work on a GPU and use Google colab?\n",
    "\n",
    "# Example training call (assuming train_loader is defined)\n",
    "train_model(model_small, train_loader, criterion, optimizer, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_small = predict(model_small, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 15:45:22,281 [INFO]: Accuracy: 0.3543046357615894\n",
      "2024-02-19 15:45:22,281 [INFO]: Accuracy: 0.3543046357615894\n",
      "2024-02-19 15:45:22,282 [INFO]: F1 Scores by class: [0.27328244 0.24938474 0.45592705]\n",
      "2024-02-19 15:45:22,282 [INFO]: F1 Scores by class: [0.27328244 0.24938474 0.45592705]\n"
     ]
    }
   ],
   "source": [
    "fscore_small = f1_score(labels, predictions_small, average=None)\n",
    "accuracy_small = accuracy_score(labels, predictions_small)\n",
    "logger.info(f\"Accuracy: {accuracy_small}\")\n",
    "logger.info(f\"F1 Scores by class: {fscore_small}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
